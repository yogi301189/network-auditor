# Network Auditor — GitHub Actions Workflow
# ==========================================
# This file tells GitHub to run the audit automatically.
# Location must be exactly: .github/workflows/audit.yml
#
# Two triggers:
#   1. Scheduled — every day at 06:00 UTC (cron syntax)
#   2. Manual    — you can press "Run workflow" in the GitHub UI anytime

name: Network Audit

on:
   push:
     branches: [main]
   schedule:
     - cron: '0 6 * * *'       #existing daily security scan at 6am
     - cron: '0 8 * * 1'       #new: weekly cost scan every Monday 8am  # 06:00 UTC daily — adjust to your timezone if needed
   workflow_dispatch:        # enables the manual "Run workflow" button in GitHub UI

# Only allow one audit to run at a time.
# If a scan is already running and a new one triggers, the new one waits.
concurrency:
  group: network-audit
  cancel-in-progress: false

jobs:
  audit:
    name: Run Network Audit
    runs-on: ubuntu-latest  # GitHub provides this VM for free

    # These permissions let the workflow write its report back to the repo
    permissions:
      contents: write       # needed to commit the report files
      id-token: write       # needed for the AWS OIDC login (more secure than API keys)

    steps:

      # ── Step 1: Get the code ───────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ── Step 2: Set up Python ─────────────────────────────────────────────
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"      # caches pip packages between runs — speeds up the workflow

      # ── Step 3: Install dependencies ──────────────────────────────────────
      - name: Install dependencies
        run: pip install -r requirements.txt

      # ── Step 4: Authenticate to AWS ───────────────────────────────────────
      # We use OIDC (OpenID Connect) — GitHub proves its identity to AWS directly.
      # This means NO long-lived API keys stored in GitHub Secrets. Much safer.
      # See setup instructions in the README below.
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_AUDIT_ROLE_ARN }}
          aws-region: us-east-1

      # ── Step 5: Run the audit ─────────────────────────────────────────────
      # ── Step 5: Run the audit ─────────────────────────────────────────────
      - name: Run Network Audit
        run: |
          # 1. Ensure we are in the root
          cd ${{ github.workspace }}
          
          # 2. Add the current root to Python's search path
          export PYTHONPATH=$PYTHONPATH:.
          
          # 3. Run it as a module (use lowercase 'auditor' if the folder is lowercase)
          python3 -m auditor.main

      # ── Step 6: Save the report as a build artifact ───────────────────────
      # GitHub keeps this file for 30 days. You can download it from the
      # Actions tab in your repo — useful for historical comparison.
      - name: Upload report artifact
        uses: actions/upload-artifact@v4
        if: always()    # upload even if the audit found violations
        with:
          name: audit-report-${{ github.run_number }}
          path: reports/
          retention-days: 30

      # ── Step 7: Commit the report back to the repo ────────────────────────
      # This creates a permanent history of every scan directly in Git.
      # You can see the trend over time just by looking at the reports/ folder.
      - name: Commit report to repository
        if: always()
        run: |
          git config user.name  "Network Auditor Bot"
          git config user.email "auditor@github-actions"
          mkdir -p reports
          git add reports/ || true
          # Only commit if there are actual changes (new report files)
          git diff --staged --quiet || git commit -m "audit: report for run #${{ github.run_number }}"
          git push || true

      # ── Step 8: Post to Slack (only if violations found) ──────────────────
      # Reads the latest summary.md and posts it to Slack.
      # Only fires when there are findings — no noise on clean days.
      - name: Post to Slack
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          # Skip entirely if the secret isn't configured yet
          if [ -z "$SLACK_WEBHOOK_URL" ]; then
            echo "SLACK_WEBHOOK_URL not configured — skipping notification."
            exit 0
          fi

          # Find the most recently written summary file
          SUMMARY_FILE=$(ls -t reports/summary_*.md 2>/dev/null | head -1)

          if [ -z "$SUMMARY_FILE" ]; then
            echo "No summary file found — skipping Slack notification."
            exit 0
          fi

          # Post first 6 lines (the header block) to Slack
          SUMMARY=$(head -6 "$SUMMARY_FILE" | sed 's/"/\\"/g' | tr '\n' ' ')

          curl -s -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "{\"text\": \"$SUMMARY\"}"
  cost-audit:
    name: Cost Waste Scan
    runs-on: ubuntu-latest
    # Run weekly on Monday mornings — cost waste doesn't change hourly
    # Also runs on every push to main so you see it in PRs
    if: github.event_name == 'push' || github.event.schedule == '0 8 * * 1'

    permissions:
      id-token: write   # needed for OIDC auth to AWS
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Configure AWS credentials (Auditor account)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_AUDIT_ROLE_ARN }}
          aws-region: us-east-1

      - name: Run Cost Optimizer
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          PYTHONPATH: .
        run: |
          python - <<'EOF'
          import json, boto3
          from pathlib import Path
          from auditor import cost_optimizer, cost_report

          # Load accounts
          with open("accounts.json") as f:
              accounts = json.load(f)

          all_cost_findings = []

          for account in accounts:
              role_arn = account.get("role_arn")
              if role_arn:
                  sts = boto3.client("sts")
                  creds = sts.assume_role(
                      RoleArn=role_arn,
                      RoleSessionName=f"CostAudit-{account['account_id']}"
                  )["Credentials"]
                  session = boto3.Session(
                      aws_access_key_id=creds["AccessKeyId"],
                      aws_secret_access_key=creds["SecretAccessKey"],
                      aws_session_token=creds["SessionToken"],
                  )
              else:
                  session = boto3.Session()

              ec2 = session.client("ec2", region_name="us-east-1")
              regions = [r["RegionName"] for r in ec2.describe_regions(
                  Filters=[{"Name": "opt-in-status", "Values": ["opt-in-not-required", "opted-in"]}]
              )["Regions"]]

              for region in regions:
                  for check in cost_optimizer.COST_CHECKS:
                      try:
                          findings = check(region, session=session)
                          for f in findings:
                              f["account_id"]   = account["account_id"]
                              f["account_name"] = account["account_name"]
                          all_cost_findings.extend(findings)
                      except Exception as e:
                          print(f"[WARN] {check.__name__} failed in {region}: {e}")

          cost_report.generate(all_cost_findings)
          EOF

      - name: Upload cost waste reports
        uses: actions/upload-artifact@v4
        with:
          name: cost-waste-reports-${{ github.run_number }}
          path: reports/cost_waste_*
          retention-days: 90   # keep 3 months of cost history
